instance_specs:
  max_price_usd_hour: 1.0 
  memory_gb: 16

work_dir: "llama.cpp" # assuming models dir is populated w/ only a quantized ggml model

setup:
    run:
    - "cd llama.cpp && make -j" # might have to make again if a different arch 
task:
  run:
    - "cd llama.cpp && ./server -m models/7B/ggml-model-q4_0.bin -c 2048" # if we've sent a quantized model already over ssh, can just start the server!